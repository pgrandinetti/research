<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Research @ 360</title>
    <link>https://pgrandinetti.github.io/research/post/</link>
    <description>Recent content in Posts on Research @ 360</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Feb 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="https://pgrandinetti.github.io/research/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>How To Compute Word Embeddings with pure Optimization</title>
      <link>https://pgrandinetti.github.io/research/2020/02/how-to-compute-word-embeddings-with-pure-optimization/</link>
      <pubDate>Sat, 15 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pgrandinetti.github.io/research/2020/02/how-to-compute-word-embeddings-with-pure-optimization/</guid>
      <description>Word embeddings are one of the most important achievements in Natural Language Processing (NLP) of recent years. Unlike the tinier and tinier percentage improvement achieved on some famous task, that are usually the highlight of many papers, word embeddings are a conceptual achievement.
Today, they are so well-known that every more word of mine about them would be quite useless. But just to set the pace: the words embeddings are vectors of real numbers in a very high dimensional space (everywhere from 50 on), such that words that are &amp;ldquo;related&amp;rdquo; have similar embeddings.</description>
    </item>
    
    <item>
      <title>Make the most out of Your MOOC</title>
      <link>https://pgrandinetti.github.io/research/2020/01/make-the-most-out-of-your-mooc/</link>
      <pubDate>Sun, 19 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pgrandinetti.github.io/research/2020/01/make-the-most-out-of-your-mooc/</guid>
      <description>Have you ever been disappointed with a MOOC?
I have been, more than once, even with very high-quality courses.
But&amp;hellip; why? As someone who has completed more than 50 MOOCs, I learnt that to ask questions is the best way to learn. Then again, why the disappointment?
I think the key part is &amp;ldquo;high-quality&amp;rdquo; MOOC. My disappointment wasn&amp;rsquo;t with the quality of the lectures (outstanding), nor with the quality with the courses&amp;rsquo; platform (excellent).</description>
    </item>
    
    <item>
      <title>Real-Time Feedback from Tensorflow Generative Model</title>
      <link>https://pgrandinetti.github.io/research/2020/01/real-time-feedback-from-tensorflow-generative-model/</link>
      <pubDate>Sun, 12 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pgrandinetti.github.io/research/2020/01/real-time-feedback-from-tensorflow-generative-model/</guid>
      <description>This article tells my research that took off after reading some recent research papers on Plug &amp;amp; Play Generative models.
Generative Deep Network Models can be powerful: they are trained to take a (small) sequence of data in input and to predict what the next element of the sequence should be.
The most famous applications of such models are in Natural Language Processing (NLP). When given a sequence of words, Generative Networks can predict the next word.</description>
    </item>
    
  </channel>
</rss>